import os
import sys
import warnings
from glob import glob

# from dependencies
import numpy as np
from scipy.io import wavfile
from sklearn.externals import joblib

from ..utils import timestamp, write_select_config
from ..audiofileIO import Spectrogram, make_syl_spects
from .feature_dicts import single_syl_features_switch_case_dict
from .feature_dicts import multiple_syl_features_switch_case_dict
from .feature_dicts import neural_net_features_switch_case_dict


class FeatureExtractor:
    def __init__(self,
                 spect_params,
                 feature_list,
                 feature_list_group_ID=None,
                 feature_group_ID_dict=None,
                 segment_params=None):
        """
        
        Parameters
        ----------
        spect_params : dict
            parameters used to create spectrograms from audio files
            as defined for hvc.audiofileIO.Spectrogram class
        feature_list : list
            list of features to extract.
            supplied by user or generated by hvc.parse.extract
        segment_params : dict
            parameters used to find segments--i.e. syllables--in audio files
            as defined for hvc.audiofileIO.segment_song
        """
        self.spect_params = spect_params
        self.spectrogram_maker = Spectrogram(spect_params)
        self.segment_params = segment_params
        self.feature_list = feature_list
        if feature_list_group_ID:
            self.feature_list_group_ID = feature_list_group_ID
            self.feature_group_ID_dict = feature_group_ID_dict

    def extract(self,
                labelset='all',
                data_dirs=None,
                data_dirs_validated=False,
                file_format=None,
                annotation_file=None,
                output_dir=None,
                make_summary_file=True):
        """loops through data dirs,
        extracts features, saves feature files, and then
        makes summary feature file.

        Parameters
        ----------
        data_dirs : list
            list of directories with data files
        output_dir :
        labelset : str
            set of labels for syllables from which features should be extracted
            specified as one string, e.g., 'iabd' would be the set {'i', 'a', 'b', 'd'}.
            Features will not be extracted from segments with labels not in this set.
            Default is 'all' in which features are extracted from all segments regardless of label.
        make_summary_file : bool
            if True, combine feature files from each directory to make a summary file
        """    
        # get absolute path to output
        # **before** we change directories
        # so we're putting it where user specified, if user wrote a relative path in config file
        if output_dir:
            output_dir = 'extract_output_' + timestamp()
            output_dir_with_path = os.path.join(
                os.path.normpath(todo['output_dir']),
                output_dir)
            if not os.path.isdir(output_dir_with_path):
                os.makedirs(output_dir_with_path)
            self.output_dir = output_dir_with_path

        if data_dirs:
            if data_dirs_validated is False:
                validated_data_dirs = []
                for data_dir in data_dirs:
                    cwd = os.getcwd()
                    if not os.path.isdir(data_dir):
                        # if item is not absolute path to dir
                        # try adding item to absolute path to config_file
                        # i.e. assume it is written relative to config file
                        data_dir = os.path.join(
                            os.path.dirname(cwd),
                            os.path.normpath(data_dir))
                        if not os.path.isdir(item):
                            raise ValueError('directory {} in data_dirs is not a valid directory.'
                                             .format(data_dir))
                    validated_data_dirs.append(data_dir)
                data_dirs = validated_data_dirs

            songfiles_list = []
            for data_dir in data_dirs:
                os.chdir(data_dir)
                if extract_params['file_format'] == 'evtaf':
                    songfiles_this_dir = glob('*.cbin')
                elif extract_params['file_format'] == 'koumura':
                    songfiles_this_dir = glob('*.wav')
                songfiles_this_dir = [os.path.abspath(songfile)
                                      for songfile in songfiles_this_dir]
                songfiles_list.extend(songfiles_this_dir)
        elif 'annotation_file' in extract_params:
            pass

        if file_format:
            if file_format == 'evtaf':
                if 'evfuncs' not in sys.modules:
                    from .. import evfuncs
            elif file_format == 'koumura':
                if 'koumura' not in sys.modules:
                    from .. import koumura
        else:
            extensions = [songfile[-4:] for songfile in songfiles_list]
            unique_extensions = set(extensions)
            if len(unique_extensions) > 1:
                raise ValueError
            if all([songfile.endswith('.cbin') for songfile in songfiles_list]):
                file_format = 'evtaf'
                if 'evfuncs' not in sys.modules:
                    from .. import evfuncs
            elif all([songfile.endswith('.wav') for songfile in songfiles_list]):
                file_format = 'wav'

        if 'features_from_all_files' in locals():
            # from last time through loop
            # (need to re-initialize for each directory)
            del features_from_all_files

        if 'neuralnet_inputs_all_files' in locals():
            del neuralnet_inputs_all_files

        num_songfiles = len(songfiles_list)
        all_labels = []
        all_onsets_s = []
        all_onsets_Hz = []
        all_offsets_s = []
        all_offsets_Hz = []
        songfile_IDs = []
        songfile_ID_counter = 0
        for file_num, songfile in enumerate(songfiles_list):
            print('Processing audio file {} of {}.'.format(file_num + 1, num_songfiles))
            # segment_params defined for todo_list item takes precedence over any default
            # defined for `extract` config
            extract_dict = self._from_file(**annotation_dict,
                                           labelset)

            if extract_dict is None:
                # because no labels from labels_to_use were found in songfile
                continue

            if 'feature_inds' in extract_dict:
                if 'feature_inds' not in locals():
                    feature_inds = extract_dict['feature_inds']
                else:
                    ftr_inds_err_msg = "feature indices changed between files"
                    assert np.array_equal(feature_inds, extract_dict['feature_inds']), ftr_inds_err_msg

            all_labels.extend(extract_dict['labels'])
            all_onsets_s.extend(extract_dict['onsets_s'])
            all_onsets_Hz.extend(extract_dict['onsets_Hz'])
            all_offsets_s.extend(extract_dict['offsets_s'])
            all_offsets_Hz.extend(extract_dict['offsets_Hz'])
            songfile_IDs.extend(
                [songfile_ID_counter] * extract_dict['onsets_s'].shape[0])
            songfile_ID_counter += 1

            if 'features_arr' in extract_dict:
                if 'features_from_all_files' in locals():
                    features_from_all_files = np.concatenate((features_from_all_files,
                                                              extract_dict['features_arr']),
                                                             axis=0)
                else:
                    features_from_all_files = extract_dict['features_arr']

            if 'neuralnet_inputs_dict' in extract_dict:
                if 'neuralnet_inputs_all_files' in locals():
                    for key, val in neuralnet_inputs_all_files.items():
                        new_val = np.concatenate((neuralnet_inputs_all_files[key],
                                                  extract_dict['neuralnet_inputs_dict'][key]))
                        neuralnet_inputs_all_files[key] = new_val
                else:
                    neuralnet_inputs_all_files = extract_dict['neuralnet_inputs_dict']

        feature_file = os.path.join(output_dir,
                                    'features_created_' + timestamp())
        feature_file_dict = {
            'labels': all_labels,
            'onsets_s': np.asarray(all_onsets_s),
            'onsets_Hz': np.asarray(all_onsets_Hz),
            'offsets_s': np.asarray(all_offsets_s),
            'offsets_Hz': np.asarray(all_offsets_Hz),
            'feature_list': extract_params['feature_list'],
            'spect_params': extract_params['spect_params'],
            'segment_params': extract_params['segment_params'],
            'labelset': extract_params['labelset'],
            'file_format': extract_params['file_format'],
            'bird_ID': extract_params['bird_ID'],
            'songfile_IDs': songfile_IDs,
            'songfiles': songfiles_list
        }

        if 'features_from_all_files' in locals():
            feature_file_dict['features'] = features_from_all_files
            feature_file_dict['features_arr_column_IDs'] = feature_inds
            num_samples = feature_file_dict['features'].shape[0]
            feature_file_dict['num_samples'] = num_samples

            if 'feature_list_group_ID' in extract_params:
                feature_file_dict['feature_list_group_ID'] = extract_params['feature_list_group_ID']
                feature_file_dict['feature_group_ID_dict'] = extract_params['feature_group_ID_dict']

        if 'neuralnet_inputs_all_files' in locals():
            feature_file_dict['neuralnet_inputs'] = neuralnet_inputs_all_files
            if 'num_samples' in feature_file_dict:
                # because we computed it for non-neural net features already
                pass
            else:
                if len(feature_file_dict['neuralnet_inputs']) == 1:
                    key = list(
                        feature_file_dict['neuralnet_inputs'].keys()
                    )[0]
                    num_samples = feature_file_dict['neuralnet_inputs'][key].shape[0]
                    feature_file_dict['num_samples'] = num_samples
                else:
                    raise ValueError('can\'t determine number of samples '
                                     'in neuralnet_inputs because there\'s '
                                     'more than one key in dictionary.')

        joblib.dump(feature_file_dict,
                    feature_file,
                    compress=3)

        ##########################################################
        # after looping through all data_dirs for this todo_item #
        ##########################################################

        if make_summary_file:
            print('making summary file')
            os.chdir(output_dir)
            summary_filename = 'summary_feature_file_created_' + timestamp()
            summary_filename_with_path = os.path.join(output_dir,
                                                      summary_filename)
            ftr_output_files = glob('features_created_*')
            if len(ftr_output_files) > 1:
                # make a 'summary' data file
                list_of_output_dicts = []
                summary_ftr_file_dict = {}
                for feature_file in ftr_output_files:
                    feature_file_dict = joblib.load(feature_file)

                    if 'labels' not in summary_ftr_file_dict:
                        summary_ftr_file_dict['labels'] = feature_file_dict['labels']
                    else:
                        summary_ftr_file_dict['labels'] = \
                            summary_ftr_file_dict['labels'] + feature_file_dict['labels']

                    if 'onsets_s' not in summary_ftr_file_dict:
                        summary_ftr_file_dict['onsets_s'] = feature_file_dict['onsets_s']
                    else:
                        summary_ftr_file_dict['onsets_s'] = \
                            np.concatenate((summary_ftr_file_dict['onsets_s'],
                                            feature_file_dict['onsets_s']))

                    if 'onsets_Hz' not in summary_ftr_file_dict:
                        summary_ftr_file_dict['onsets_Hz'] = feature_file_dict['onsets_Hz']
                    else:
                        summary_ftr_file_dict['onsets_Hz'] = \
                            np.concatenate((summary_ftr_file_dict['onsets_Hz'],
                                            feature_file_dict['onsets_Hz']))

                    if 'offsets_s' not in summary_ftr_file_dict:
                        summary_ftr_file_dict['offsets_s'] = feature_file_dict['offsets_s']
                    else:
                        summary_ftr_file_dict['offsets_s'] = \
                            np.concatenate((summary_ftr_file_dict['offsets_s'],
                                            feature_file_dict['offsets_s']))

                    if 'offsets_Hz' not in summary_ftr_file_dict:
                        summary_ftr_file_dict['offsets_Hz'] = feature_file_dict['offsets_Hz']
                    else:
                        summary_ftr_file_dict['offsets_Hz'] = \
                            np.concatenate((summary_ftr_file_dict['offsets_Hz'],
                                            feature_file_dict['offsets_Hz']))

                    if 'spect_params' not in summary_ftr_file_dict:
                        summary_ftr_file_dict['spect_params'] = feature_file_dict['spect_params']
                    else:
                        if feature_file_dict['spect_params'] != summary_ftr_file_dict['spect_params']:
                            raise ValueError('mismatch between spect_params in {} '
                                             'and other feature files'.format(feature_file))

                    if 'segment_params' not in summary_ftr_file_dict:
                        summary_ftr_file_dict['segment_params'] = feature_file_dict['segment_params']
                    else:
                        if feature_file_dict['segment_params'] != summary_ftr_file_dict['segment_params']:
                            raise ValueError('mismatch between segment_params in {} '
                                             'and other feature files'.format(feature_file))

                    if 'labelset' not in summary_ftr_file_dict:
                        summary_ftr_file_dict['labelset'] = feature_file_dict['labelset']
                    else:
                        if feature_file_dict['labelset'] != summary_ftr_file_dict['labelset']:
                            raise ValueError('mismatch between labelset in {} '
                                             'and other feature files'.format(feature_file))

                    if 'file_format' not in summary_ftr_file_dict:
                        summary_ftr_file_dict['file_format'] = feature_file_dict['file_format']
                    else:
                        if feature_file_dict['file_format'] != summary_ftr_file_dict['file_format']:
                            raise ValueError('mismatch between file_format in {} '
                                             'and other feature files'.format(feature_file))

                    if 'bird_ID' not in summary_ftr_file_dict:
                        summary_ftr_file_dict['bird_ID'] = feature_file_dict['bird_ID']
                    else:
                        if feature_file_dict['bird_ID'] != summary_ftr_file_dict['bird_ID']:
                            raise ValueError('mismatch between bird_ID in {} '
                                             'and other feature files'.format(feature_file))

                    if 'songfile_IDs' not in summary_ftr_file_dict:
                        summary_ftr_file_dict['songfile_IDs'] = feature_file_dict['songfile_IDs']
                    else:
                        new_1st_ID = len(summary_ftr_file_dict['songfile_IDs'])  # works because of 0 indexing
                        tmp_songfile_IDs = [ID + new_1st_ID for ID in feature_file_dict['songfile_IDs']]
                        summary_ftr_file_dict['songfile_IDs'].extend(tmp_songfile_IDs)

                    if 'songfiles' not in summary_ftr_file_dict:
                        summary_ftr_file_dict['songfiles'] = feature_file_dict['songfiles']
                    else:
                        summary_ftr_file_dict['songfiles'].extend(feature_file_dict['songfiles'])

                    if 'feature_list' not in summary_ftr_file_dict:
                        summary_ftr_file_dict['feature_list'] = feature_file_dict['feature_list']
                    else:
                        if feature_file_dict['feature_list'] != summary_ftr_file_dict['feature_list']:
                            raise ValueError('mismatch between feature_list in {} '
                                             'and other feature files'.format(feature_file))

                    # only if not-neuralnet features were used
                    if 'features' in feature_file_dict:
                        if 'features' not in summary_ftr_file_dict:
                            summary_ftr_file_dict['features'] = feature_file_dict['features']
                        else:
                            summary_ftr_file_dict['features'] = np.concatenate((summary_ftr_file_dict['features'],
                                                                                feature_file_dict['features']))

                        if 'features_arr_column_IDs' not in summary_ftr_file_dict:
                            summary_ftr_file_dict['features_arr_column_IDs'] = feature_file_dict[
                                'features_arr_column_IDs']
                        else:
                            if any(feature_file_dict['features_arr_column_IDs'] !=
                                           summary_ftr_file_dict['features_arr_column_IDs']):
                                raise ValueError('mismatch between features_arr_column_IDs in {} '
                                                 'and other feature files'.format(feature_file))

                        if 'feature_list_group_ID' in extract_params:
                            if 'feature_list_group_ID' not in summary_ftr_file_dict:
                                summary_ftr_file_dict['feature_list_group_ID'] = feature_file_dict[
                                    'feature_list_group_ID']
                            else:
                                if feature_file_dict['feature_list_group_ID'] != \
                                        summary_ftr_file_dict['feature_list_group_ID']:
                                    raise ValueError('mismatch between feature_list_group_ID in {} '
                                                     'and other feature files'.format(feature_file))

                            if 'feature_group_ID_dict' not in summary_ftr_file_dict:
                                summary_ftr_file_dict['feature_group_ID_dict'] = \
                                    feature_file_dict['feature_group_ID_dict']
                            else:
                                if feature_file_dict['feature_group_ID_dict'] != \
                                        summary_ftr_file_dict['feature_group_ID_dict']:
                                    raise ValueError('mismatch between feature_group_ID_dict in {} '
                                                     'and other feature files'.format(feature_file))

                    # if extracting inputs for neuralnets
                    if 'neuralnet_inputs' in feature_file_dict:
                        if 'neuralnet_inputs' not in summary_ftr_file_dict:
                            summary_ftr_file_dict['neuralnet_inputs'] = \
                                feature_file_dict['neuralnet_inputs']
                        else:
                            for key, val in summary_ftr_file_dict['neuralnet_inputs'].items():
                                newval = np.concatenate((summary_ftr_file_dict['neuralnet_inputs'][key],
                                                         feature_file_dict['neuralnet_inputs'][key]))
                                summary_ftr_file_dict['neuralnet_inputs'][key] = newval

                if 'features' in summary_ftr_file_dict:
                    summary_ftr_file_dict['num_samples'] = \
                        summary_ftr_file_dict['features'].shape[0]
                elif 'neuralnet_inputs' in summary_ftr_file_dict:
                    if len(summary_ftr_file_dict['neuralnet_inputs']) == 1:
                        key = list(
                            summary_ftr_file_dict['neuralnet_inputs'].keys()
                        )[0]
                        num_samples = summary_ftr_file_dict['neuralnet_inputs'][key].shape[0]
                        summary_ftr_file_dict['num_samples'] = num_samples
                    else:
                        raise ValueError('can\'t determine number of samples '
                                         'in neuralnet_inputs because there\'s '
                                         'more than one key in dictionary.')

                joblib.dump(summary_ftr_file_dict,
                            summary_filename)

            else:  # if only one feature_file
                os.rename(ftr_output_files[0],
                          summary_filename)
                summary_ftr_file_dict = joblib.load(summary_filename)

            if 'feature_list_group_ID' in summary_ftr_file_dict:
                write_select_config(summary_ftr_file_dict,
                                    summary_filename,
                                    output_dir)

    def _from_file(self,
                   filename,
                   labels_to_use,
                   song_labels,
                   onsets,
                   offsets,
                   file_format=None,
                  ):
        """
        extracts features from an audio file containing birdsong

        Parameters
        ----------
        filename : str
            audio file
        labels_to_use : str
            either string of labels, e.g., 'iabcdef' or '012345'
            or 'all'
            Defined in config file, generated by hvc.parse.extract
        file_format : str
            'evtaf' or 'koumura'
    
        Returns
        -------
        extract_dict : dict
            with following keys:
                labels : list of chars
                    of length m, one label for each syllable in features_arr
                    Always returned.
                features_arr : m-by-n numpy array
                    where each column n is a feature or one element of a multi-column feature
                    (e.g. spectrum is a multi-column feature)
                    and each row m represents one syllable
                    Returned for all non-"neuralnet input" features.
                feature_inds : 1-d numpy array of ints
                    indexing array used by hvc/extract to split feature_arr back up into
                    feature groups
                    Array will be of length n where n is number of columns in features_arr,
                    but unique(feature_inds) = len(feature_list)
                    Returned for all non-"neuralnet input" features.
                neuralnet_inputs_dict : dict
                    dict where keys are names of a neuralnet model and value is corresponding
                    input for each model, e.g., 2-d array containing spectrogram
        """

        if filename.endswith('.cbin'):
            raw_audio, samp_freq = evfuncs.load_cbin(filename)
        elif filename.endswith('.wav'):
            samp_freq, raw_audio = wavfile.read(filename)

        labels_to_use = np.asarray([label_to_use in song_labels
                                    for label_to_use in labels_to_use])

        if not np.any(labels_to_use):
            warnings.warn('No labels in {0} matched labels to use: {1}\n'
                          'Did not extract features from file.'
                          .format(filename, labelset))
            return None


    
    
        # initialize indexing array for features
        # used to split back up into feature groups
        feature_inds = []
    
        # loop through features first instead of syls because
        # some features do not require making spectrogram
        ########################################################################
        # so how this loop works is, make an array of length syllables, and for#
        # each syllable calculate the feature and then insert the values in    #
        # the corresponding index. After looping through all syllables,        #
        # concatenate w/growing features array.                                #
        ########################################################################
        for ftr_ind, current_feature in enumerate(self.feature_list):
            # if this is a feature extracted from a single syllable, i.e.,
            # if this feature requires a spectrogram
            if current_feature in single_syl_features_switch_case_dict:
                if 'syl_spects' not in locals():
                    syl_spects = make_syl_spects(raw_audio,
                                                 samp_freq,
                                                 self.spectrogram_maker,
                                                 song_labels,
                                                 labels_to_use,
                                                 onsets,
                                                 offsets)
                if 'curr_feature_arr' in locals():
                    del curr_feature_arr

                for ind, syl in enumerate(syl_spects):
                    # extract current feature from every syllable
                    if syl.spect is np.nan:
                        # can't extract feature so leave as nan
                        continue
                    ftr = single_syl_features_switch_case_dict[current_feature](syl)

                    if 'curr_feature_arr' in locals():
                        if np.isscalar(ftr):
                            curr_feature_arr[ind] = ftr
                        else:
                            # note have to add dimension with newaxis because np.concat requires
                            # same number of dimensions, but extract_features returns 1d.
                            # Decided to keep it explicit that we go to 2d here.
                            curr_feature_arr[ind, :] = ftr[np.newaxis, :]
                    else:  # if curr_feature_arr doesn't exist yet
                        # initialize vector, if feature is a scalar, or matrix, if feature is a vector
                        # where each element (scalar feature) or row (vector feature) is feature from
                        # one syllable.
                        # Initialize as nan so that if there are syllables from which feature could
                        # not be extracted, the value for that feature stays as nan
                        # (e.g. because segment was too short to make spectrogram
                        # with given spectrogram values)
                        if np.isscalar(ftr):
                            curr_feature_arr = np.full((len(song.syls)), np.nan)
                            # may not be on first syllable if first spectrogram was nan
                            # so need to index into initialized array
                            curr_feature_arr[ind] = ftr
                        else:
                            curr_feature_arr = np.full((len(song.syls),
                                                        ftr.shape[-1]), np.nan)
                            # may not be on first syllable if first spectrogram was nan
                            # so need to index into initialized array
                            curr_feature_arr[ind, :] = ftr[np.newaxis, :]  # make 2-d for concatenate

                # after looping through all syllables:
                if 'features_arr' in locals():
                    if np.isscalar(ftr):
                        # if feature is scalar,
                        # then `ftr` from all syllables will be a (row) vector
                        # so transpose to column vector then add to growing end of 2d matrix
                        feature_inds.extend([ftr_ind])
                        features_arr = np.concatenate((features_arr,
                                                       curr_feature_arr[np.newaxis, :].T),
                                                      axis=1)
                    else:
                        # if feature is not scalar,
                        # `ftr` will be 2-d, so don't transpose before you concatenate
                        feature_inds.extend([ftr_ind] * ftr.shape[-1])
                        features_arr = np.concatenate((features_arr,
                                                       curr_feature_arr),
                                                      axis=1)
                else:  # if 'features_arr' doesn't exist yet
                    if np.isscalar(ftr):
                        feature_inds.extend([ftr_ind])
                    else:
                        feature_inds.extend([ftr_ind] * ftr.shape[-1])
                    features_arr = curr_feature_arr

            elif current_feature in multiple_syl_features_switch_case_dict:
                syls_to_use = np.asarray([label in ])
                curr_feature_arr = multiple_syl_features_switch_case_dict[current_feature](onsets,
                                                                                           offsets,
                                                                                           labels_to_use)
                feature_inds.extend([ftr_ind])
                if 'features_arr' in locals():
                    features_arr = np.concatenate((features_arr,
                                                   curr_feature_arr[:, np.newaxis]),
                                                  axis=1)
                else:
                    features_arr = curr_feature_arr[:, np.newaxis]
            elif current_feature in neural_net_features_switch_case_dict:
                curr_neuralnet_input = neural_net_features_switch_case_dict[current_feature](song,
                                                                                             spect_params)
                if 'neuralnet_inputs_dict' in locals():
                    if current_feature in neuralnet_inputs_dict:
                        if type(neuralnet_inputs_dict[current_feature]) is np.ndarray:
                            neuralnet_inputs_dict[current_feature] = \
                                np.concatenate((neuralnet_inputs_dict[current_feature],
                                                curr_neuralnet_input),
                                               axis=-1)
                    else:
                        neuralnet_inputs_dict[current_feature] = curr_neuralnet_input
                else:
                    neuralnet_inputs_dict = {current_feature: curr_neuralnet_input}

        labels = [label for label in song.labels if label in labels_to_use]
        # return extract dict that has labels and features_arr and/or neuralnet_inputs_dict
        extract_dict = {'labels': labels}
        extract_dict['onsets_s'] = song.onsets_s[song.syls_to_use]
        extract_dict['onsets_Hz'] = song.onsets_Hz[song.syls_to_use]
        extract_dict['offsets_s'] = song.offsets_s[song.syls_to_use]
        extract_dict['offsets_Hz'] = song.offsets_Hz[song.syls_to_use]
        if 'features_arr' in locals():
            extract_dict['features_arr'] = features_arr
            extract_dict['feature_inds'] = np.asarray(feature_inds)
        if 'neuralnet_inputs_dict' in locals():
            extract_dict['neuralnet_inputs_dict'] = neuralnet_inputs_dict
        return extract_dict
