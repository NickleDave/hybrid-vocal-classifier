import os
import sys
import warnings
from glob import glob

# from dependencies
import numpy as np
from scipy.io import wavfile
from sklearn.externals import joblib

from ..utils import timestamp, write_select_config, annotation
from ..audiofileIO import Spectrogram, Segmenter, make_syls
from .feature_dicts import single_syl_features_switch_case_dict
from .feature_dicts import multiple_syl_features_switch_case_dict
from .feature_dicts import neural_net_features_switch_case_dict
from .. import evfuncs
from ..koumura import to_csv


class FeatureExtractor:
    def __init__(self,
                 spect_params,
                 feature_list,
                 feature_list_group_ID=None,
                 feature_group_ID_dict=None,
                 segment_params=None):
        """
        Parameters
        ----------
        spect_params : dict
            parameters used to create spectrograms from audio files
            as defined for hvc.audiofileIO.Spectrogram class
        feature_list : list
            list of features to extract.
            supplied by user or generated by hvc.parse.extract
        segment_params : dict
            parameters used to find segments--i.e. syllables--in audio files
            as defined for hvc.audiofileIO.segment_song
        """
        self.spect_params = spect_params
        self.spectrogram_maker = Spectrogram(**self.spect_params)
        if segment_params:
            self.segment_params = segment_params
            self.segmenter = Segmenter(**self.segment_params)
        self.feature_list = feature_list
        if feature_list_group_ID:
            self.feature_list_group_ID = feature_list_group_ID
            self.feature_group_ID_dict = feature_group_ID_dict

    def extract(self,
                labelset='all',
                data_dirs=None,
                data_dirs_validated=False,
                file_format=None,
                annotation_file=None,
                segment=False,
                output_dir=None,
                make_summary_file=True):
        """extract features and save feature files

        Parameters
        ----------
        data_dirs : list
            list of directories with data files
        output_dir :
        labelset : str
            set of labels for syllables from which features should be extracted
            specified as one string, e.g., 'iabd' would be the set {'i', 'a', 'b', 'd'}.
            Features will not be extracted from segments with labels not in this set.
            Default is 'all' in which features are extracted from all segments regardless of label.
        make_summary_file : bool
            if True, combine feature files from each directory to make a summary file
        """
        if data_dirs and annotation_file:
            raise ValueError('received values for both data_dirs and '
                             'annotation_file arguments, unclear which to use. '
                             'Please only specify one or the other.')

        # get absolute path to output
        # **before** we change directories
        # so we're putting it where user specified, if user wrote a relative path in config file
        if output_dir:
            output_subdir = 'extract_output_' + timestamp()
            output_dir_with_path = os.path.join(
                os.path.abspath(
                    os.path.normpath(output_dir)),
                output_subdir)
            if not os.path.isdir(output_dir_with_path):
                os.makedirs(output_dir_with_path)
            output_dir = output_dir_with_path

        if data_dirs:
            if data_dirs_validated is False:
                validated_data_dirs = []
                for data_dir in data_dirs:
                    cwd = os.getcwd()
                    if not os.path.isdir(data_dir):
                        # if item is not absolute path to dir
                        # try adding item to absolute path to config_file
                        # i.e. assume it is written relative to config file
                        data_dir = os.path.join(
                            os.path.dirname(cwd),
                            os.path.normpath(data_dir))
                        if not os.path.isdir(data_dir):
                            raise ValueError('directory {} in data_dirs is not a valid directory.'
                                             .format(data_dir))
                    validated_data_dirs.append(data_dir)
                data_dirs = validated_data_dirs

            # try to auto-discover file format
            if file_format is None:
                os.chdir(data_dirs[0])
                cbins = glob('*.cbin')
                wavs = glob('*.wav')
                if cbins and wavs:
                    raise ValueError('Could not determine file format for feature extract automatically,'
                                     'found more than one valid format in {}.'
                                     .format(data_dirs[0]))
                elif cbins and wavs==[]:
                    print('found .cbin files in {}, will use .cbin as file format'
                          .format(data_dirs[0]))
                    file_format = 'cbin'
                elif wavs and cbins==[]:
                    file_format = 'wav'
                    print('found .wav files in {}, will use .wav as file format'
                          .format(data_dirs[0]))

            if segment is False:
                # if we are not segmenting songs (e.g. for prediction of unlabeled song)
                # but user passed data_dir instead of annotation_file,
                # look for annotation files (for now just .not.mat files)
                notmats = []
                annot_xmls = []
                for data_dir in data_dirs:
                    notmat_search_str = os.path.join(data_dir, '*.not.mat')
                    notmats_this_dir = glob(notmat_search_str)
                    if notmats_this_dir:
                        notmats.extend(notmats_this_dir)
                    else:
                        if file_format == 'cbin':
                            # if audio files are .cbin, we expect .not.mat files, so raise error
                            raise ValueError('Identified file format as .cbin but did not find '
                                             'files with annotations in data_dir {}.'
                                             .format(data_dir))
                        elif file_format == 'wav':
                            # if audio files are .wav, annotation could be xml files (from Koumura dataset),
                            # try looking in parent directory
                            annot_xml = glob(os.path.join(data_dir,
                                                          '..',
                                                          'Annotation.xml'))
                            if annot_xml:
                                annot_xmls.append(annot_xml)
                            else:
                                raise ValueError('Identified file format as .wav but did not find '
                                                 'files with annotations in data_dir {}.'
                                                 .format(data_dir))

                annotation_list = []  # list of annotation_dicts
                if notmats:
                    for notmat in notmats:
                        annotation_dict = annotation.notmat_to_annotat_dict(notmat)
                        annotation_list.append(annotation_dict)

                if annot_xmls:
                    for annot_xml in annot_xmls:
                        annotation_csv = to_csv(annot_xml)
                        annotation_list.extend(annotation.csv_to_list(annotation_csv))

        # if user passed argument for annotation_file, not data_dirs
        elif annotation_file:
            annotation_csv = annotation.load_annotation_csv(annotation_file)
            # load annotation file
            # then convert to annotation_list
            annotation_list = annotation.csv_to_list(annotation_csv)

        num_songfiles = len(annotation_list)
        all_labels = []
        all_onsets_Hz = []
        all_offsets_Hz = []
        songfile_IDs = []
        songfile_ID_counter = 0
        for file_num, annotation_dict in enumerate(annotation_list):
            print('Processing audio file {} of {}.'.format(file_num + 1, num_songfiles))
            # segment_params defined for todo_list item takes precedence over any default
            # defined for `extract` config
            extract_dict = self._from_file(**annotation_dict,
                                           labels_to_use=labelset)

            if extract_dict is None:
                # because no labels from labels_to_use were found in songfile
                continue

            if 'feature_inds' in extract_dict:
                if 'feature_inds' not in locals():
                    feature_inds = extract_dict['feature_inds']
                else:
                    ftr_inds_err_msg = "feature indices changed between files"
                    assert np.array_equal(feature_inds, extract_dict['feature_inds']), ftr_inds_err_msg

            all_labels.extend(extract_dict['labels'])
            all_onsets_Hz.extend(extract_dict['onsets_Hz'])
            all_offsets_Hz.extend(extract_dict['offsets_Hz'])
            songfile_IDs.extend(
                [songfile_ID_counter] * extract_dict['onsets_Hz'].shape[0])
            songfile_ID_counter += 1

            if 'features_arr' in extract_dict:
                if 'features_from_all_files' in locals():
                    features_from_all_files = np.concatenate((features_from_all_files,
                                                              extract_dict['features_arr']),
                                                             axis=0)
                else:
                    features_from_all_files = extract_dict['features_arr']

            if 'neuralnet_inputs_dict' in extract_dict:
                if 'neuralnet_inputs_all_files' in locals():
                    for key, val in neuralnet_inputs_all_files.items():
                        new_val = np.concatenate((neuralnet_inputs_all_files[key],
                                                  extract_dict['neuralnet_inputs_dict'][key]))
                        neuralnet_inputs_all_files[key] = new_val
                else:
                    neuralnet_inputs_all_files = extract_dict['neuralnet_inputs_dict']

        feature_file = os.path.join(output_dir,
                                    'features_created_' + timestamp())
        feature_file_dict = {
            'labels': all_labels,
            'onsets_Hz': np.asarray(all_onsets_Hz),
            'offsets_Hz': np.asarray(all_offsets_Hz),
            'feature_list': self.feature_list,
            'spect_params': self.spect_params,
            'segment_params': self.segment_params,
            'labelset': labelset,
            'file_format': file_format,
            'songfile_IDs': songfile_IDs,
            'annotation_list': annotation_list
        }

        if 'features_from_all_files' in locals():
            feature_file_dict['features'] = features_from_all_files
            feature_file_dict['features_arr_column_IDs'] = feature_inds
            num_samples = feature_file_dict['features'].shape[0]
            feature_file_dict['num_samples'] = num_samples

            if hasattr(self, 'feature_list_group_ID'):
                feature_file_dict['feature_list_group_ID'] = self.feature_list_group_ID
                feature_file_dict['feature_group_ID_dict'] = self.feature_group_ID_dict

        if 'neuralnet_inputs_all_files' in locals():
            feature_file_dict['neuralnet_inputs'] = neuralnet_inputs_all_files
            if 'num_samples' in feature_file_dict:
                # because we computed it for non-neural net features already
                pass
            else:
                if len(feature_file_dict['neuralnet_inputs']) == 1:
                    key = list(
                        feature_file_dict['neuralnet_inputs'].keys()
                    )[0]
                    num_samples = feature_file_dict['neuralnet_inputs'][key].shape[0]
                    feature_file_dict['num_samples'] = num_samples
                else:
                    raise ValueError('can\'t determine number of samples '
                                     'in neuralnet_inputs because there\'s '
                                     'more than one key in dictionary.')

        joblib.dump(feature_file_dict,
                    feature_file,
                    compress=3)

        ##########################################################
        # after looping through all data_dirs for this todo_item #
        ##########################################################

        if make_summary_file:
            print('making summary file')
            os.chdir(output_dir)
            summary_filename = 'summary_feature_file_created_' + timestamp()
            summary_filename_with_path = os.path.join(output_dir,
                                                      summary_filename)
            ftr_output_files = glob('features_created_*')
            if len(ftr_output_files) > 1:
                # make a 'summary' data file
                list_of_output_dicts = []
                summary_ftr_file_dict = {}
                for feature_file in ftr_output_files:
                    feature_file_dict = joblib.load(feature_file)

                    if 'labels' not in summary_ftr_file_dict:
                        summary_ftr_file_dict['labels'] = feature_file_dict['labels']
                    else:
                        summary_ftr_file_dict['labels'] = \
                            summary_ftr_file_dict['labels'] + feature_file_dict['labels']

                    if 'onsets_Hz' not in summary_ftr_file_dict:
                        summary_ftr_file_dict['onsets_Hz'] = feature_file_dict['onsets_Hz']
                    else:
                        summary_ftr_file_dict['onsets_Hz'] = \
                            np.concatenate((summary_ftr_file_dict['onsets_Hz'],
                                            feature_file_dict['onsets_Hz']))

                    if 'offsets_Hz' not in summary_ftr_file_dict:
                        summary_ftr_file_dict['offsets_Hz'] = feature_file_dict['offsets_Hz']
                    else:
                        summary_ftr_file_dict['offsets_Hz'] = \
                            np.concatenate((summary_ftr_file_dict['offsets_Hz'],
                                            feature_file_dict['offsets_Hz']))

                    if 'spect_params' not in summary_ftr_file_dict:
                        summary_ftr_file_dict['spect_params'] = feature_file_dict['spect_params']
                    else:
                        if feature_file_dict['spect_params'] != summary_ftr_file_dict['spect_params']:
                            raise ValueError('mismatch between spect_params in {} '
                                             'and other feature files'.format(feature_file))

                    if 'segment_params' not in summary_ftr_file_dict:
                        summary_ftr_file_dict['segment_params'] = feature_file_dict['segment_params']
                    else:
                        if feature_file_dict['segment_params'] != summary_ftr_file_dict['segment_params']:
                            raise ValueError('mismatch between segment_params in {} '
                                             'and other feature files'.format(feature_file))

                    if 'labelset' not in summary_ftr_file_dict:
                        summary_ftr_file_dict['labelset'] = feature_file_dict['labelset']
                    else:
                        if feature_file_dict['labelset'] != summary_ftr_file_dict['labelset']:
                            raise ValueError('mismatch between labelset in {} '
                                             'and other feature files'.format(feature_file))

                    if 'file_format' not in summary_ftr_file_dict:
                        summary_ftr_file_dict['file_format'] = feature_file_dict['file_format']
                    else:
                        if feature_file_dict['file_format'] != summary_ftr_file_dict['file_format']:
                            raise ValueError('mismatch between file_format in {} '
                                             'and other feature files'.format(feature_file))

                    if 'bird_ID' not in summary_ftr_file_dict:
                        summary_ftr_file_dict['bird_ID'] = feature_file_dict['bird_ID']
                    else:
                        if feature_file_dict['bird_ID'] != summary_ftr_file_dict['bird_ID']:
                            raise ValueError('mismatch between bird_ID in {} '
                                             'and other feature files'.format(feature_file))

                    if 'songfile_IDs' not in summary_ftr_file_dict:
                        summary_ftr_file_dict['songfile_IDs'] = feature_file_dict['songfile_IDs']
                    else:
                        new_1st_ID = len(summary_ftr_file_dict['songfile_IDs'])  # works because of 0 indexing
                        tmp_songfile_IDs = [ID + new_1st_ID for ID in feature_file_dict['songfile_IDs']]
                        summary_ftr_file_dict['songfile_IDs'].extend(tmp_songfile_IDs)

                    if 'songfiles' not in summary_ftr_file_dict:
                        summary_ftr_file_dict['songfiles'] = feature_file_dict['songfiles']
                    else:
                        summary_ftr_file_dict['songfiles'].extend(feature_file_dict['songfiles'])

                    if 'feature_list' not in summary_ftr_file_dict:
                        summary_ftr_file_dict['feature_list'] = feature_file_dict['feature_list']
                    else:
                        if feature_file_dict['feature_list'] != summary_ftr_file_dict['feature_list']:
                            raise ValueError('mismatch between feature_list in {} '
                                             'and other feature files'.format(feature_file))

                    # only if not-neuralnet features were used
                    if 'features' in feature_file_dict:
                        if 'features' not in summary_ftr_file_dict:
                            summary_ftr_file_dict['features'] = feature_file_dict['features']
                        else:
                            summary_ftr_file_dict['features'] = np.concatenate((summary_ftr_file_dict['features'],
                                                                                feature_file_dict['features']))

                        if 'features_arr_column_IDs' not in summary_ftr_file_dict:
                            summary_ftr_file_dict['features_arr_column_IDs'] = feature_file_dict[
                                'features_arr_column_IDs']
                        else:
                            if any(feature_file_dict['features_arr_column_IDs'] !=
                                           summary_ftr_file_dict['features_arr_column_IDs']):
                                raise ValueError('mismatch between features_arr_column_IDs in {} '
                                                 'and other feature files'.format(feature_file))

                        if hasattr(self, 'feature_list_group_ID'):
                            if 'feature_list_group_ID' not in summary_ftr_file_dict:
                                summary_ftr_file_dict['feature_list_group_ID'] = feature_file_dict[
                                    'feature_list_group_ID']
                            else:
                                if feature_file_dict['feature_list_group_ID'] != \
                                        summary_ftr_file_dict['feature_list_group_ID']:
                                    raise ValueError('mismatch between feature_list_group_ID in {} '
                                                     'and other feature files'.format(feature_file))

                            if 'feature_group_ID_dict' not in summary_ftr_file_dict:
                                summary_ftr_file_dict['feature_group_ID_dict'] = \
                                    feature_file_dict['feature_group_ID_dict']
                            else:
                                if feature_file_dict['feature_group_ID_dict'] != \
                                        summary_ftr_file_dict['feature_group_ID_dict']:
                                    raise ValueError('mismatch between feature_group_ID_dict in {} '
                                                     'and other feature files'.format(feature_file))

                    # if extracting inputs for neuralnets
                    if 'neuralnet_inputs' in feature_file_dict:
                        if 'neuralnet_inputs' not in summary_ftr_file_dict:
                            summary_ftr_file_dict['neuralnet_inputs'] = \
                                feature_file_dict['neuralnet_inputs']
                        else:
                            for key, val in summary_ftr_file_dict['neuralnet_inputs'].items():
                                newval = np.concatenate((summary_ftr_file_dict['neuralnet_inputs'][key],
                                                         feature_file_dict['neuralnet_inputs'][key]))
                                summary_ftr_file_dict['neuralnet_inputs'][key] = newval

                if 'features' in summary_ftr_file_dict:
                    summary_ftr_file_dict['num_samples'] = \
                        summary_ftr_file_dict['features'].shape[0]
                elif 'neuralnet_inputs' in summary_ftr_file_dict:
                    if len(summary_ftr_file_dict['neuralnet_inputs']) == 1:
                        key = list(
                            summary_ftr_file_dict['neuralnet_inputs'].keys()
                        )[0]
                        num_samples = summary_ftr_file_dict['neuralnet_inputs'][key].shape[0]
                        summary_ftr_file_dict['num_samples'] = num_samples
                    else:
                        raise ValueError('can\'t determine number of samples '
                                         'in neuralnet_inputs because there\'s '
                                         'more than one key in dictionary.')

                joblib.dump(summary_ftr_file_dict,
                            summary_filename)

            else:  # if only one feature_file
                os.rename(ftr_output_files[0],
                          summary_filename)
                summary_ftr_file_dict = joblib.load(summary_filename)

            if 'feature_list_group_ID' in summary_ftr_file_dict:
                write_select_config(summary_ftr_file_dict,
                                    summary_filename,
                                    output_dir)

    def _from_file(self,
                   filename,
                   labels,
                   onsets_Hz,
                   offsets_Hz,
                   labels_to_use,
                   file_format=None,
                   ):
        """
        extracts features from an audio file containing birdsong

        Parameters
        ----------
        filename : str
            audio file
        labels_to_use : str
            either string of labels, e.g., 'iabcdef' or '012345'
            or 'all'
            Defined in config file, generated by hvc.parse.extract
        file_format : str
            'evtaf' or 'koumura'

        Returns
        -------
        extract_dict : dict
            with following keys:
                labels : list of chars
                    of length m, one label for each syllable in features_arr
                    Always returned.
                features_arr : m-by-n numpy array
                    where each column n is a feature or one element of a multi-column feature
                    (e.g. spectrum is a multi-column feature)
                    and each row m represents one syllable
                    Returned for all non-"neuralnet input" features.
                feature_inds : 1-d numpy array of ints
                    indexing array used by hvc/extract to split feature_arr back up into
                    feature groups
                    Array will be of length n where n is number of columns in features_arr,
                    but unique(feature_inds) = len(feature_list)
                    Returned for all non-"neuralnet input" features.
                neuralnet_inputs_dict : dict
                    dict where keys are names of a neuralnet model and value is corresponding
                    input for each model, e.g., 2-d array containing spectrogram
        """
        if filename.endswith('.cbin'):
            raw_audio, samp_freq = evfuncs.load_cbin(filename)
        elif filename.endswith('.wav'):
            samp_freq, raw_audio = wavfile.read(filename)

        labels_to_use = np.asarray([label in labels_to_use
                                    for label in labels])
        labels = np.asarray(list(labels))

        if not np.any(labels_to_use):
            warnings.warn('No labels in {0} matched labels to use: {1}\n'
                          'Did not extract features from file.'
                          .format(filename, labelset))
            return None

        # initialize indexing array for features
        # used to split back up into feature groups
        feature_inds = []
    
        # loop through features first instead of syls because
        # some features do not require making spectrogram
        ########################################################################
        # so how this loop works is, make an array of length syllables, and for#
        # each syllable calculate the feature and then insert the values in    #
        # the corresponding index. After looping through all syllables,        #
        # concatenate w/growing features array.                                #
        ########################################################################
        for ftr_ind, current_feature in enumerate(self.feature_list):
            # if this is a feature extracted from a single syllable, i.e.,
            # if this feature requires a spectrogram
            if current_feature in single_syl_features_switch_case_dict:
                if 'syls' not in locals():
                    syls = make_syls(raw_audio,
                                     samp_freq,
                                     self.spectrogram_maker,
                                     labels[labels_to_use],
                                     onsets_Hz[labels_to_use],
                                     offsets_Hz[labels_to_use])
                if 'curr_feature_arr' in locals():
                    del curr_feature_arr

                for ind, syl in enumerate(syls):
                    # extract current feature from every syllable
                    if syl.spect is np.nan:
                        # can't extract feature so leave as nan
                        continue
                    ftr = single_syl_features_switch_case_dict[current_feature](syl)

                    if 'curr_feature_arr' in locals():
                        if np.isscalar(ftr):
                            curr_feature_arr[ind] = ftr
                        else:
                            # note have to add dimension with newaxis because np.concat requires
                            # same number of dimensions, but extract_features returns 1d.
                            # Decided to keep it explicit that we go to 2d here.
                            curr_feature_arr[ind, :] = ftr[np.newaxis, :]
                    else:  # if curr_feature_arr doesn't exist yet
                        # initialize vector, if feature is a scalar, or matrix, if feature is a vector
                        # where each element (scalar feature) or row (vector feature) is feature from
                        # one syllable.
                        # Initialize as nan so that if there are syllables from which feature could
                        # not be extracted, the value for that feature stays as nan
                        # (e.g. because segment was too short to make spectrogram
                        # with given spectrogram values)
                        if np.isscalar(ftr):
                            curr_feature_arr = np.full((len(syls)), np.nan)
                            # may not be on first syllable if first spectrogram was nan
                            # so need to index into initialized array
                            curr_feature_arr[ind] = ftr
                        else:
                            curr_feature_arr = np.full((len(syls),
                                                        ftr.shape[-1]), np.nan)
                            # may not be on first syllable if first spectrogram was nan
                            # so need to index into initialized array
                            curr_feature_arr[ind, :] = ftr[np.newaxis, :]  # make 2-d for concatenate

                # after looping through all syllables:
                if 'features_arr' in locals():
                    if np.isscalar(ftr):
                        # if feature is scalar,
                        # then `ftr` from all syllables will be a (row) vector
                        # so transpose to column vector then add to growing end of 2d matrix
                        feature_inds.extend([ftr_ind])
                        features_arr = np.concatenate((features_arr,
                                                       curr_feature_arr[np.newaxis, :].T),
                                                      axis=1)
                    else:
                        # if feature is not scalar,
                        # `ftr` will be 2-d, so don't transpose before you concatenate
                        feature_inds.extend([ftr_ind] * ftr.shape[-1])
                        features_arr = np.concatenate((features_arr,
                                                       curr_feature_arr),
                                                      axis=1)
                else:  # if 'features_arr' doesn't exist yet
                    if np.isscalar(ftr):
                        feature_inds.extend([ftr_ind])
                    else:
                        feature_inds.extend([ftr_ind] * ftr.shape[-1])
                    features_arr = curr_feature_arr

            elif current_feature in multiple_syl_features_switch_case_dict:
                curr_feature_arr = multiple_syl_features_switch_case_dict[current_feature](onsets_Hz,
                                                                                           offsets_Hz,
                                                                                           labels_to_use)
                feature_inds.extend([ftr_ind])
                if 'features_arr' in locals():
                    features_arr = np.concatenate((features_arr,
                                                   curr_feature_arr[:, np.newaxis]),
                                                  axis=1)
                else:
                    features_arr = curr_feature_arr[:, np.newaxis]
            elif current_feature in neural_net_features_switch_case_dict:
                curr_neuralnet_input = neural_net_features_switch_case_dict[current_feature](song,
                                                                                             spect_params)
                if 'neuralnet_inputs_dict' in locals():
                    if current_feature in neuralnet_inputs_dict:
                        if type(neuralnet_inputs_dict[current_feature]) is np.ndarray:
                            neuralnet_inputs_dict[current_feature] = \
                                np.concatenate((neuralnet_inputs_dict[current_feature],
                                                curr_neuralnet_input),
                                               axis=-1)
                    else:
                        neuralnet_inputs_dict[current_feature] = curr_neuralnet_input
                else:
                    neuralnet_inputs_dict = {current_feature: curr_neuralnet_input}

        # return extract dict that has labels and features_arr and/or neuralnet_inputs_dict
        extract_dict = {'labels': labels[labels_to_use]}
        extract_dict['onsets_Hz'] = onsets_Hz[labels_to_use]
        extract_dict['offsets_Hz'] = offsets_Hz[labels_to_use]
        if 'features_arr' in locals():
            extract_dict['features_arr'] = features_arr
            extract_dict['feature_inds'] = np.asarray(feature_inds)
        if 'neuralnet_inputs_dict' in locals():
            extract_dict['neuralnet_inputs_dict'] = neuralnet_inputs_dict
        return extract_dict
